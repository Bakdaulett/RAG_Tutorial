import json
from pydantic_ai import Agent


class LLMJudge:
    """
    LLM Judge that performs semantic comparison between generated response and true response.
    Uses Gemini to evaluate if the responses are semantically equivalent.
    """

    def __init__(self, model):
        """
        Initialize LLM Judge.

        Args:
            model: Gemini model instance from pydantic_ai GoogleModel
        """
        self.model = model
        self.agent = Agent(model=model)

    def evaluate(self, llm_response: str, true_response: str, query: str = None) -> dict:
        """
        Evaluate if LLM response is semantically equivalent to true response.

        Args:
            llm_response: The response generated by the system
            true_response: The ground truth/expected response
            query: Optional original query for context

        Returns:
            dict with 'judgment' (True/False), 'confidence', and 'explanation'
        """

        query_context = f"\nOriginal Query: {query}\n" if query else ""

        prompt = f"""You are an expert judge evaluating AI-generated responses for semantic equivalence.

{query_context}
Generated Response:
{llm_response}

True/Expected Response:
{true_response}

Evaluate whether these two responses are semantically equivalent. They don't need to be word-for-word identical, but should:
1. Convey the same core information
2. Have the same factual accuracy
3. Answer the same question with the same meaning
4. Not contradict each other on key points

Minor differences in wording, style, or detail level are acceptable as long as the core meaning is preserved.

Respond in JSON format:
{{
    "judgment": true or false,
    "confidence": 0.0 to 1.0,
    "explanation": "brief explanation of your judgment"
}}

Only output valid JSON, nothing else."""

        try:
            response = self.agent.run_sync(prompt)

            # Parse JSON from response (pydantic-ai returns result as string directly)
            response_text = str(response.output).strip() if hasattr(response, 'output') else str(response).strip()

            # Extract JSON if wrapped in markdown
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()

            result = json.loads(response_text)

            # Validate judgment
            if "judgment" not in result:
                print("Invalid judgment format, defaulting to False")
                return {
                    "judgment": False,
                    "confidence": 0.0,
                    "explanation": "Invalid response format"
                }

            # Ensure confidence is present
            if "confidence" not in result:
                result["confidence"] = 0.5

            return result

        except Exception as e:
            print(f"Error in LLM judge: {e}")
            return {
                "judgment": False,
                "confidence": 0.0,
                "explanation": f"Error in evaluation: {str(e)}"
            }

    def judge(self, llm_response: str, true_response: str, query: str = None) -> bool:
        """
        Simple boolean judgment.

        Args:
            llm_response: The response generated by the system
            true_response: The ground truth/expected response
            query: Optional original query for context

        Returns:
            True if responses are semantically equivalent, False otherwise
        """
        result = self.evaluate(llm_response, true_response, query)
        return result["judgment"]